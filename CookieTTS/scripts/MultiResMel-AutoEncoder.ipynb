{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Band Auto-Encoder\n",
    "### This Network should learn to compress the information from multiple 2400 channel spectrograms of different window lengths into a low dimensional latent space.\n",
    "\n",
    "### - This latent space can then be used to train a `Text -> Feature -> Vocoder` setup in place of typical Mel-Spectrograms.\n",
    "\n",
    "### - The latent space should contain multiple windows worth of information, and potentially encode the types of noise occuring in the frame.\n",
    "\n",
    "---\n",
    "\n",
    "- ### Updated to add iso226 volume scaling for features. (noticed far too much model focus on 11Khz+ channels)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1 - Install ISO226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import iso226\n",
    "except:\n",
    "    !git clone https://github.com/jacobbaylesssmc/iso226\n",
    "    !cd iso226; python3 -m pip install ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Import Dependancies/Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"LRU_CACHE_CAPACITY\"] = \"3\"\n",
    "import random\n",
    "\n",
    "from CookieTTS.utils.dataset.utils import load_wav_to_torch, load_filepaths_and_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1 - Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from CookieTTS.utils.dataset.utils import load_wav_to_torch\n",
    "from CookieTTS.utils.audio.stft import STFT\n",
    "from CookieTTS.utils.audio.audio_processing import window_sumsquare, dynamic_range_compression, dynamic_range_decompression\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_config, gpu_dataloading, directory, wildcard_filter = \"*.wav\"):\n",
    "        self.gpu_dataloading = gpu_dataloading\n",
    "        print(\"Finding Audio Files...\")\n",
    "        self.audio_files = glob( os.path.join(directory, \"**\", wildcard_filter), recursive=True)\n",
    "        print(\"Done\")\n",
    "        \n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audio_files)\n",
    "        self.len = len(self.audio_files)\n",
    "        \n",
    "        self.max_len_s = model_config['max_len_s']\n",
    "        self.win_lens = model_config['window_lengths']\n",
    "        self.hop_len = model_config['hop_length']\n",
    "        self.fil_len = model_config['filter_length']\n",
    "        \n",
    "        self.stfts = []\n",
    "        for win_len in self.win_lens:\n",
    "            stft = STFT(filter_length=self.fil_len,\n",
    "                           hop_length=self.hop_len,\n",
    "                           win_length=win_len,)\n",
    "            stft = stft.cuda() if self.gpu_dataloading else stft\n",
    "            self.stfts.append(stft)\n",
    "        \n",
    "        self.directory = directory\n",
    "        self.wildcard_filter = wildcard_filter\n",
    "    \n",
    "    def get_mel(self, audio):\n",
    "        \"\"\"Take audio and convert to multi-res spectrogram\"\"\"\n",
    "        melspec = []\n",
    "        for stft in self.stfts:\n",
    "            spect = stft.transform(audio.unsqueeze(0), return_phase=False)[0].squeeze(0)# -> [n_mel, dec_T]\n",
    "            #print(spect.shape)\n",
    "            melspec.append(spect)\n",
    "        return torch.cat(melspec, dim=0)# [[n_mel, dec_T], ...] -> [n_stft*n_mel, dec_T]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_path = self.audio_files[index]\n",
    "        audio, sampling_rate, max_mag = load_wav_to_torch(audio_path) # load mono audio from file\n",
    "        audio = audio / max_mag # normalize to range [-1, 1]\n",
    "        \n",
    "        #noisy_audio = audio.clone()\n",
    "        #noisy_audio += torch.randn(*audio.shape) * random.uniform(self.min_noise_str, self.max_noise_str)\n",
    "        #noisy_audio = noisy_audio.clamp(min=-0.999, max=0.999)\n",
    "        #noisy_spect = dynamic_range_compression(self.get_mel(noisy_audio))\n",
    "        \n",
    "        if audio.shape[0] > int(self.max_len_s*sampling_rate):\n",
    "            max_start = audio.shape[0] - int(self.max_len_s*sampling_rate)\n",
    "            start = (torch.rand(1)*max_start).int()\n",
    "            audio = audio[start:start+int(self.max_len_s*sampling_rate)]\n",
    "        \n",
    "        audio = audio.cuda() if self.gpu_dataloading else audio\n",
    "        \n",
    "        spect = self.get_mel(audio)\n",
    "        spect = dynamic_range_compression(spect)\n",
    "        \n",
    "        spect_length = spect.shape[1]\n",
    "        return (spect, spect_length)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelCollate():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        B = len(batch)\n",
    "        lengths = [x[0].shape[1] for x in batch]\n",
    "        n_mel = batch[0][0].shape[0]\n",
    "        max_length = max(*lengths)\n",
    "        b_spect = [x[0] for x in batch]\n",
    "        b_spect = torch.cat(b_spect, dim=1).unsqueeze(0)# [1, n_stft*n_mel, sum(dec_T)]\n",
    "        \n",
    "        #for i in range(B):\n",
    "        #    spect = batch[i][0]\n",
    "        #    b_spect[i, :, :spect.shape[1]] = spect\n",
    "        \n",
    "        spect_lengths = torch.tensor([sum(lengths),])\n",
    "        model_inputs = (b_spect, spect_lengths)\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 - Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_layers, n_dim, kernel_w, bias=True, act_func=nn.LeakyReLU(negative_slope=0.1, inplace=True), dropout=0.0, res=False):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            in_dim = input_dim if i == 0 else n_dim\n",
    "            out_dim = output_dim if i+1 == n_layers else n_dim\n",
    "            pad = (kernel_w - 1)//2\n",
    "            conv = nn.Conv1d(in_dim, out_dim, kernel_w, padding=pad, bias=bias)\n",
    "            self.layers.append(conv)\n",
    "        self.act_func = act_func\n",
    "        self.dropout = dropout\n",
    "        self.res = res\n",
    "        if self.res:\n",
    "            assert input_dim == output_dim, 'residual connection requires input_dim and output_dim to match.'\n",
    "    \n",
    "    def forward(self, x): # [B, in_dim, T]\n",
    "        skip = x\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            is_last_layer = bool( i+1 == len(self.layers) )\n",
    "            x = layer(x)\n",
    "            if not is_last_layer:\n",
    "                x = self.act_func(x)\n",
    "            if self.dropout > 0.0 and self.training:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training, inplace=True)\n",
    "        if self.res:\n",
    "            x += skip\n",
    "        return x # [B, out_dim, T]\n",
    "\n",
    "\n",
    "class Coder(nn.Module):\n",
    "    def __init__(self, model_config, input_dim=None, output_dim=None, output_batchnorm=False):\n",
    "        super(Coder, self).__init__()\n",
    "        self.input_dim = ((model_config['filter_length']//2) + 1) * len(model_config['window_lengths']) if input_dim is None else input_dim\n",
    "        self.output_dim = model_config['latent_dim'] if output_dim is None else output_dim\n",
    "        self.device = \"cuda\"\n",
    "        \n",
    "        self.temporalblocks = nn.ModuleList()\n",
    "        for i in range(model_config['n_blocks']):\n",
    "            b_first_block = bool(i == 0)\n",
    "            b_last_block = bool(i+1 == model_config['n_blocks'])\n",
    "            in_dim  = self.input_dim if b_first_block else model_config['n_dim']\n",
    "            out_dim = self.output_dim if b_last_block else model_config['n_dim']\n",
    "            res = True if (model_config['residual_connections'] and in_dim == out_dim) else False\n",
    "            n_layers = model_config['bottleneck_n_layers'] if b_first_block or b_last_block else model_config['n_layers']\n",
    "            temp_block = TemporalBlock(in_dim, out_dim, n_layers, model_config['n_dim'],\n",
    "                                       model_config['kernel_width'], bias = model_config['bias'],\n",
    "                                       dropout = model_config['dropout'], res=res)\n",
    "            self.temporalblocks.append(temp_block)\n",
    "        \n",
    "        if output_batchnorm:\n",
    "            self.bn_out = nn.BatchNorm1d(self.output_dim, momentum=0.05, affine=False)\n",
    "    \n",
    "    def forward(self, spect):\n",
    "        assert spect.shape[1] == self.input_dim, f'input Tensor is wrong shape ({spect.shape}). Expected {self.input_dim} channels.'\n",
    "        spect = spect.to(self.device)\n",
    "        \n",
    "        for block in self.temporalblocks:\n",
    "            spect = block(spect)\n",
    "        \n",
    "        spect = spect.clone()\n",
    "        \n",
    "        if hasattr(self, 'bn_out'):\n",
    "            spect = self.bn_out(spect)\n",
    "        return spect\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.in_out_dim = ((model_config['filter_length']//2) + 1) * len(model_config['window_lengths'])\n",
    "        self.latent_dim = model_config['latent_dim']\n",
    "        \n",
    "        self.encoder = Coder(model_config, self.in_out_dim, self.latent_dim, output_batchnorm=True)\n",
    "        self.decoder = Coder(model_config, self.latent_dim, self.in_out_dim)\n",
    "    \n",
    "    def get_specs(self, audio):\n",
    "        spects = []\n",
    "        for stft in self.stfts:\n",
    "            spect = stft.transform(audio.unsqueeze(0), return_phase=False)[0].squeeze(0)# -> [n_mel, dec_T]\n",
    "            spects.append(spect)\n",
    "        spect = torch.cat(spects, dim=0)# [[n_mel, dec_T], ...] -> [n_stft*n_mel, T//hop_len]\n",
    "        spect = dynamic_range_compression(spect)# change to clamped log-scale magnitudes\n",
    "        return spect.unsqueeze(0)# -> [1, n_stft*n_mel, T//hop_len]\n",
    "    \n",
    "    def encode_audiopath(self, audio_path):\n",
    "        audio, sampling_rate, max_mag = load_wav_to_torch(audio_path) # load mono audio from file\n",
    "        audio /= max_mag # normalize to range [-1, 1]\n",
    "        return self.encode_audio(audio)\n",
    "    \n",
    "    def encode_audio(self, audio):\n",
    "        \"\"\"Encoder [T] Tensor into Z learned latent representation.\"\"\"\n",
    "        spect = self.get_specs(audio.cuda())# -> [B, n_stfts*n_fft, T]\n",
    "        z = self.encoder(spect)# -> [B, z_dim, T]\n",
    "        return z\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        spect, spect_lengths = inputs\n",
    "        \n",
    "        z = self.encoder(spect)\n",
    "        \n",
    "        rec_spect = self.decoder(z)\n",
    "        \n",
    "        return rec_spect.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import iso226\n",
    "import math\n",
    "from CookieTTS.utils.model.utils import get_mask_from_lengths\n",
    "\n",
    "# https://www.desmos.com/calculator/4nac7kvt7p\n",
    "# Squash smaller values together so that mse loss is lower on quieter parts of the spectrogram.\n",
    "def vol_rescale_loss(mel, power=0.5, min=-11.55):\n",
    "    mel = mel + (power/(-min*2))*(mel**2)\n",
    "    return mel\n",
    "\n",
    "\n",
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(LossFunction, self).__init__()\n",
    "        iso226_spl_from_freq = iso226.iso226_spl_itpl(L_N=60, hfe=True)# get InterpolatedUnivariateSpline for Perc Sound Pressure Level at Difference Frequencies with 60DB ref.\n",
    "        self.freq_weights = torch.tensor([(2**(60./10))/(2**(iso226_spl_from_freq(freq)/10)) for freq in np.linspace(0, model_config['sampling_rate']//2, (model_config['filter_length']//2)+1)])\n",
    "        self.freq_weights = self.freq_weights.cuda().repeat(len(model_config['window_lengths']))[None, :, None]# [B, n_mel, T]\n",
    "        \n",
    "        self.loud_loss_priority_str = model_config['loud_loss_priority']\n",
    "    \n",
    "    def forward(self, y, x):\n",
    "        gt_spect, lengths = x\n",
    "        gt_spect = gt_spect.cuda()\n",
    "        pred_spect = y\n",
    "        \n",
    "        mask = get_mask_from_lengths(lengths.cuda())\n",
    "        mask = mask.expand(gt_spect.size(1), mask.size(0), mask.size(1))\n",
    "        mask = mask.permute(1, 0, 2)\n",
    "        #gt_spect.detach()[~mask] = 0.0\n",
    "        #pred_spect.detach()[~mask] = 0.0\n",
    "        \n",
    "        if self.loud_loss_priority_str > 0:\n",
    "            pred_spect = vol_rescale_loss(pred_spect, power=self.loud_loss_priority_str)\n",
    "            gt_spect = vol_rescale_loss(gt_spect, power=self.loud_loss_priority_str)\n",
    "        \n",
    "        MAE = F.mse_loss(pred_spect, gt_spect, reduction='none')\n",
    "        MAE = MAE * self.freq_weights# [B, n_mel, T] * [1, n_mel, 1]\n",
    "        MAE = torch.masked_select(MAE, mask)# [B, n_mel, T] -> [n_mel*sum(n_frames)]\n",
    "        \n",
    "        return MAE.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.9 - Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "def plot_data(data, title=None, figsize=(20, 7.5), range_=[-11.6, 2.0]):\n",
    "    \"\"\"\n",
    "    data: list([height, width], [height, width], ...)\n",
    "    \"\"\"\n",
    "    #for i in range(len(data)):\n",
    "    #    data[i][0,0] = range_[0]\n",
    "    #    data[i][0,1] = range_[1]\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
    "    for i in range(len(data)):\n",
    "        if title:\n",
    "            axes[i].set_title(title[i])\n",
    "        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
    "                       interpolation='none')\n",
    "    plt.show()\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 - Train and Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "----- Previous Models -----\n",
    "AEF1 - 160 Channels with 12*5 Coder Layers, Learned Mean/STD\n",
    "AEF4 - 160 Channels with  3*1 Coder Layers, Learned Mean/STD\n",
    "AEF5 - 512 Channels with  3*3 Coder Layers, Zero Mean Unit Variance\n",
    "AEF6 - 256 Channels with  3*3 Coder Layers, Zero Mean Unit Variance\n",
    "AEF7 - 128 Channels with  3*3 Coder Layers, Zero Mean Unit Variance\n",
    "AEF8 - 192 Channels with  3*3 Coder Layers, Zero Mean Unit Variance\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"model_name\": \"AEF8\",\n",
    "    \"audio_directory\": \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2\",#r\"H:\\ClipperDatasetV2\\SlicedDialogue\",\n",
    "    \"wildcard_filter\": \"*.wav\",\n",
    "    \"batch_size\": 8,\n",
    "    'max_len_s': 6.0,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"latent_dim\": 192,\n",
    "    \"loud_loss_priority\": 0.1,# squash smaller values so model will prioritise louder parts of the spectrogram. # 0.0 = Off, 1.0 = Nearly parts have 0.0 loss.\n",
    "    \"sampling_rate\": 48000,\n",
    "    \"window_lengths\": [600, 1200, 2400],\n",
    "    \"hop_length\": 600,\n",
    "    \"filter_length\": 2400,\n",
    "    \"n_blocks\": 1,#3,\n",
    "    \"n_layers\": 3,\n",
    "    \"bottleneck_n_layers\": 1,\n",
    "    \"n_dim\": 256,\n",
    "    \"kernel_width\": 1,\n",
    "    \"residual_connections\": True,\n",
    "    \"bias\": True,\n",
    "    \"dropout\": 0.00,\n",
    "}\n",
    "\n",
    "gpu_dataloading = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "dataset = AudioDataset(model_config, gpu_dataloading, model_config['audio_directory'], model_config['wildcard_filter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Training\n",
    "collate_fn = MelCollate()\n",
    "train_loader = DataLoader(dataset, num_workers=0 if gpu_dataloading else 12, shuffle=True,\n",
    "                              batch_size=model_config['batch_size'],\n",
    "                              pin_memory=False, drop_last=True,\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "criterion = LossFunction(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize/Reset Model\n",
    "model = AutoEncoder(model_config).cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=model_config['learning_rate'])\n",
    "\n",
    "iteration = 0\n",
    "avg_improvement = 0.0\n",
    "avg_training_loss = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_path = \"MelAutoEncoder_50000_AEF7.pt\"\n",
    "#checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "#model.load_state_dict(checkpoint_dict['model'])\n",
    "#optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "#iteration = checkpoint_dict['iteration']\n",
    "#model_config = checkpoint_dict['model_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        learning_rate = 2e-4\n",
    "        if iteration > 50000:\n",
    "            learning_rate = 1e-4\n",
    "        if iteration > 75000:\n",
    "            learning_rate = 0.5e-4\n",
    "        if iteration > 100000:\n",
    "            learning_rate = 0.25e-4\n",
    "        if iteration > 110000:\n",
    "            learning_rate = 0.125e-4\n",
    "        if iteration > 120000:\n",
    "            learning_rate = 0.0625e-4\n",
    "        \n",
    "        learning_rate *= 0.74\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        reduced_loss = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_training_loss = avg_training_loss*0.99 + reduced_loss*(1-0.99)\n",
    "        if iteration%100 == 0:\n",
    "            print(f\"\\n[Iter {iteration:<6}] [Training Loss {reduced_loss:5.3f} Avg {avg_training_loss:5.3f}]\", end='')\n",
    "        else:\n",
    "            print(\".\", end='')\n",
    "        if True and iteration%10000 == 0:\n",
    "            plot_data([x[0][:,:400].float().cpu().detach().numpy() for x in batch[0:1]]+\\\n",
    "                      [outputs[0][:,:400].float().cpu().detach().numpy()],\n",
    "                      figsize=(24, 48))\n",
    "        if iteration%25000==0:\n",
    "            filepath = \"/media/cookie/Samsung PM961/TwiBot/CookiePPPTTS/CookieTTS/scripts/MelAutoEncoder\"+f\"_{iteration}_{model_config['model_name']}.pt\"\n",
    "            saving_dict = {\n",
    "                'model': model.state_dict(),\n",
    "                'iteration': iteration,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate,\n",
    "                'model_config': model_config}\n",
    "            torch.save(saving_dict, filepath)\n",
    "        iteration+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#filepath = r\"G:\\TwiBot\\CookiePPPTTS\\CookieTTS\\scripts\\MelAutoEncoder\"+f\"_{iteration}.pt\"\n",
    "filepath = \"/media/cookie/Samsung PM961/TwiBot/CookiePPPTTS/CookieTTS/scripts/MelAutoEncoder\"+f\"_{iteration}_{model_config['model_name']}.pt\"\n",
    "\n",
    "print(f\"Saving checkpoint to '{filepath}'\")\n",
    "saving_dict = {\n",
    "    'model': model.state_dict(),\n",
    "    'iteration': iteration,\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'learning_rate': learning_rate,\n",
    "    'model_config': model_config,\n",
    "    }\n",
    "torch.save(saving_dict, filepath)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4 - Convert Dataset to new Latent features.\n",
    "\n",
    "#### `.wav` -> `.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "AEF1 - 160 Channels with 12*5 Coder Layers, Learned Mean/STD\n",
    "AEF4 - 160 Channels with  3*1 Coder Layers, Learned Mean/STD\n",
    "AEF5 - 512 Channels with  3*3 Coder Layers, Zero Mean Unit Variance\n",
    "AEF6 - 256 Channels with  3*3 Coder Layers, Zero Mean Unit Variance\n",
    "AEF7 - 128 Channels with  3*3 Coder Layers, Zero Mean Unit Variance\n",
    "AEF8 - 192 Channels with  3*3 Coder Layers, Zero Mean Unit Variance\n",
    "\n",
    "(edited)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"MelAutoEncoder_150000_AEF6.pt\"\n",
    "checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "iteration = checkpoint_dict['iteration']\n",
    "model_config = checkpoint_dict['model_config']\n",
    "\n",
    "# load model\n",
    "model = AutoEncoder(model_config).cuda()\n",
    "model.load_state_dict(checkpoint_dict['model'])\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_len = 600\n",
    "# update as needed!\n",
    "\n",
    "model.stfts = []\n",
    "for win_len in model_config['window_lengths']:\n",
    "    stft = STFT(filter_length=model_config['filter_length'],\n",
    "                hop_length=hop_len, win_length=win_len,).cuda()\n",
    "    model.stfts.append(stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "directory = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    audiopaths = glob( os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "    len_audiopaths = len(audiopaths)\n",
    "    for i, audiopath in enumerate(audiopaths):\n",
    "        latent_z = model.encode_audiopath(audiopath).squeeze(0)\n",
    "        print(f'{i:6}/{len_audiopaths:<6} {latent_z.shape}', end='\\r')\n",
    "        new_save_path = audiopath.replace('.wav','.npy')\n",
    "        #print(torch.from_numpy(np.load(new_save_path)).shape)\n",
    "        np.save(new_save_path, latent_z.data.squeeze().float().cpu().numpy())\n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
