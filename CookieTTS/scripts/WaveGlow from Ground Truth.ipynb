{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CookieTTS.utils.audio.stft import TacotronSTFT, STFT\n",
    "from CookieTTS.utils.dataset.utils import load_wav_to_torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../_4_mtw/waveglow') # add WaveGlow to System path for easier importing\n",
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Initialize WaveGlow and Load Checkpoint/Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WaveGlow\n",
    "import json\n",
    "\n",
    "waveglow_path = r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_6_Flow_512C_ssvae2\\best_val_model\"\n",
    "config_fpath = r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_6_Flow_512C_ssvae2\\config.json\"\n",
    "\n",
    "def is_ax(config):\n",
    "    \"\"\"Quickly check if a model uses the Ax WaveGlow core by what's available in the config file.\"\"\"\n",
    "    return True if 'upsample_first' in config.keys() else False\n",
    "\n",
    "# Load config file\n",
    "with open(config_fpath) as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "train_config = config[\"train_config\"]\n",
    "data_config = config[\"data_config\"]\n",
    "if 'preempthasis' not in data_config.keys():\n",
    "    data_config['preempthasis'] = 0.0\n",
    "dist_config = config[\"dist_config\"]\n",
    "data_config['n_mel_channels'] = config[\"waveglow_config\"]['n_mel_channels'] if 'n_mel_channels' in config[\"waveglow_config\"].keys() else 160\n",
    "waveglow_config = {\n",
    "    **config[\"waveglow_config\"], \n",
    "    'win_length': data_config['win_length'],\n",
    "    'hop_length': data_config['hop_length'],\n",
    "    'preempthasis': data_config['preempthasis']\n",
    "}\n",
    "print(waveglow_config)\n",
    "print(f\"Config File from '{config_fpath}' successfully loaded.\")\n",
    "\n",
    "# import the correct model core\n",
    "if is_ax(waveglow_config):\n",
    "    from CookieTTS._4_mtw.waveglow.efficient_model_ax import WaveGlow\n",
    "else:\n",
    "    if waveglow_config[\"yoyo\"]:\n",
    "        from CookieTTS._4_mtw.waveglow.efficient_model import WaveGlow\n",
    "    else:\n",
    "        from CookieTTS._4_mtw.waveglow.glow import WaveGlow\n",
    "from CookieTTS._4_mtw.waveglow.denoiser import Denoiser\n",
    "\n",
    "# initialize model\n",
    "print(f\"intializing WaveGlow model... \", end=\"\")\n",
    "waveglow = WaveGlow(**waveglow_config).cuda()\n",
    "print(f\"Done!\")\n",
    "\n",
    "# load checkpoint from file\n",
    "print(f\"loading WaveGlow checkpoint... \", end=\"\")\n",
    "checkpoint = torch.load(waveglow_path)\n",
    "waveglow.load_state_dict(checkpoint['model']) # and overwrite initialized weights with checkpointed weights\n",
    "waveglow.cuda().eval().half() # move to GPU and convert to half precision\n",
    "waveglow.remove_weightnorm()\n",
    "print(f\"Done!\")\n",
    "\n",
    "print(f\"initializing Denoiser... \", end=\"\")\n",
    "denoiser = Denoiser(waveglow, mu=0., var=2.0, stft_device='cpu', speaker_dependant=False)\n",
    "print(f\"Done!\")\n",
    "waveglow_iters = checkpoint['iteration']\n",
    "print(f\"WaveGlow trained for {waveglow_iters} iterations\")\n",
    "speaker_lookup = checkpoint['speaker_lookup'] # ids lookup\n",
    "training_sigma = train_config['sigma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Setup STFT to generate wavs from audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for generating Spectrograms from Audio files\n",
    "print('Initializing STFT...')\n",
    "stft = TacotronSTFT(data_config['filter_length'], data_config['hop_length'], data_config['win_length'],\n",
    "                    data_config['n_mel_channels'], data_config['sampling_rate'], data_config['mel_fmin'],\n",
    "                    data_config['mel_fmax'])\n",
    "print('Done!')\n",
    "def load_mel(path):\n",
    "    if path.endswith('.wav') or path.endswith('.flac'):\n",
    "        audio, sampling_rate, max_audio_value = load_wav_to_torch(path)\n",
    "        if sampling_rate != stft.sampling_rate:\n",
    "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "                sampling_rate, stft.sampling_rate))\n",
    "        audio_norm = audio / max_audio_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = stft.mel_spectrogram(audio_norm)\n",
    "    elif path.endswith('.npy'):\n",
    "        melspec = torch.from_numpy(np.load(path))\n",
    "    else:\n",
    "        pass\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Reconstruct Audio from Audio Spectrogram using WaveGlow/Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S1\\s1e26\"\n",
    "ext = '.mel.npy'\n",
    "sigmas = [0.95,]\n",
    "denoise_strengths = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 3.0]\n",
    "\n",
    "speaker_ids = [167,]\n",
    "speaker_ids = [speaker_lookup[x] for x in speaker_ids] # map speaker ids to internel\n",
    "speaker_ids = torch.tensor(speaker_ids).cuda().long()\n",
    "\n",
    "display_audio = False\n",
    "display_denoised_audio = False\n",
    "\n",
    "save_outputs = True\n",
    "output_folder = r'D:\\Downloads\\infer\\WaveFlow\\AR_6_Flow_512C_ssvae2'\n",
    "\n",
    "for audio_path in glob(os.path.join(folder_path, '**', f'*{ext}'), recursive=True):\n",
    "    print(f\"Audio Path:\\n'{audio_path}'\\n\")\n",
    "    mel_outputs_postnet = load_mel(audio_path).cuda()\n",
    "    output_path = os.path.join(output_folder,os.path.splitext(os.path.split(audio_path)[-1])[0])\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    audios = []\n",
    "    save_path = os.path.join(output_path, 'Ground Truth.wav')\n",
    "    shutil.copy(audio_path.replace('.mel.npy','.wav'), save_path)\n",
    "    with torch.no_grad():\n",
    "        for i, sigma in enumerate(sigmas):\n",
    "            with torch.random.fork_rng(devices=[0,]):\n",
    "                torch.random.manual_seed(0)# use same Z / random seed during validation so results are more consistent and comparable.\n",
    "                audio = waveglow.infer(mel_outputs_postnet, sigma=sigma, speaker_ids=speaker_ids, return_CPU=True).float()\n",
    "                if (torch.isnan(audio) | torch.isinf(audio)).any():\n",
    "                    print('inf or nan found in audio')\n",
    "                audio[torch.isnan(audio) | torch.isinf(audio)] = 0.0\n",
    "                #audio[:,-1]=1.0\n",
    "                audios.append(audio)\n",
    "                print(f\"sigma = {sigma}\")\n",
    "                if display_audio:\n",
    "                    ipd.display(ipd.Audio(audio[0].data.cpu().numpy(), rate=data_config['sampling_rate']))\n",
    "                if save_outputs:\n",
    "                    save_path = os.path.join(output_path, f'denoise_{0.00:0.2f}_sigma_{sigma:0.2f}.wav')\n",
    "                    write(save_path, data_config['sampling_rate'], (audio[0]* 2**15).data.cpu().numpy().astype('int16'))\n",
    "        \n",
    "        for i, (audio, sigma) in enumerate(zip(audios, sigmas)):\n",
    "            for denoise_strength in denoise_strengths:\n",
    "                audio_denoised = denoiser(audio, speaker_ids=speaker_ids, strength=denoise_strength)[:, 0]\n",
    "                if (torch.isnan(audio) | torch.isinf(audio)).any():\n",
    "                    print('inf or nan found in audio')\n",
    "                assert (not torch.isinf(audio_denoised).any()) or (not torch.isnan(audio_denoised).any())\n",
    "                print(f\"[Denoised Strength {denoise_strength}] [sigma {sigma}]\")\n",
    "                if display_denoised_audio:\n",
    "                    ipd.display(ipd.Audio(audio_denoised.cpu().numpy(), rate=data_config['sampling_rate']))\n",
    "                if save_outputs:\n",
    "                    save_path = os.path.join(output_path, f'denoise_{denoise_strength:0.2f}_sigma_{sigma:0.2f}.wav')\n",
    "                    write(save_path, data_config['sampling_rate'], (audio_denoised[0]* 2**15).data.cpu().numpy().astype('int16'))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Testing) Dynamic Time Warping for GTA Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.rand(1, 2, 700)\n",
    "pred = torch.rand(1, 2, 700)\n",
    "\n",
    "@torch.jit.script\n",
    "def DTW(batch_pred, batch_target, scale_factor: int, range_: int):\n",
    "    \"\"\"\n",
    "    Calcuates ideal time-warp for each frame to minimize L1 Error from target.\n",
    "    Params:\n",
    "        scale_factor: Scale factor for linear interpolation.\n",
    "                      Values greater than 1 allows blends neighbouring frames to be used.\n",
    "        range_: Range around the target frame that predicted frames should be tested as possible candidates to output.\n",
    "                If range is set to 1, then predicted frames with more than 0.5 distance cannot be used. (where 0.5 distance means blending the 2 frames together).\n",
    "    \"\"\"\n",
    "    assert range_ % 2 == 1, 'range_ must be an odd integer.'\n",
    "    assert batch_pred.shape == batch_target.shape, 'pred and target shapes do not match.'\n",
    "    \n",
    "    batch_pred_dtw = batch_pred * 0.\n",
    "    for i, (pred, target) in enumerate(zip(batch_pred, batch_target)):\n",
    "        pred = pred.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "        \n",
    "        # shift pred into all aligned forms that might produce improved L1\n",
    "        pred_pad = torch.nn.functional.pad(pred, (range_//2, range_//2))\n",
    "        pred_expanded = torch.nn.functional.interpolate(pred_pad, scale_factor=float(scale_factor), mode='linear', align_corners=False)# [B, C, T] -> [B, C, T*s]\n",
    "        \n",
    "        p_shape = pred.shape\n",
    "        pred_list = []\n",
    "        for j in range(scale_factor*range_):\n",
    "            pred_list.append(pred_expanded[:,:,j::scale_factor][:,:,:p_shape[2]])\n",
    "        \n",
    "        pred_dtw = pred.clone()\n",
    "        for pred_interpolated in pred_list:\n",
    "            new_l1 = torch.nn.functional.l1_loss(pred_interpolated, target, reduction='none').sum(dim=1, keepdim=True)\n",
    "            old_l1 = torch.nn.functional.l1_loss(pred_dtw, target, reduction='none').sum(dim=1, keepdim=True)\n",
    "            pred_dtw = torch.where(new_l1 < old_l1, pred_interpolated, pred_dtw)\n",
    "        batch_pred_dtw[i:i+1] = pred_dtw\n",
    "    return batch_pred_dtw\n",
    "\n",
    "pred_dtw = DTW(pred, target, 4, 3)\n",
    "print(torch.nn.functional.l1_loss(pred, target))\n",
    "print(torch.nn.functional.l1_loss(pred_dtw, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "def plot_data(data, title=None, figsize=(20, 5)):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.imshow(data, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "filetext = open(r\"G:\\TwiBot\\CookiePPPTTS\\CookieTTS\\_2_ttm\\tacotron2\\GTA_flist\\map_train.txt\", \"r\").read().split(\"\\n\")\n",
    "filter_str = [\".mel100\",\".mel200\",\".mel300\",\".mel400\",\".mel500\"]\n",
    "filetext = [x for x in filetext if not any(str_ in x for str_ in filter_str)]\n",
    "\n",
    "rand_start = int(random.random()*len(filetext))\n",
    "file_count = 10\n",
    "for line in filetext[rand_start:rand_start+file_count]:\n",
    "    pred_mel_path = line.split(\"|\")[1].replace(\"\\n\",\"\").replace(\"/media/cookie/Samsung 860 QVO/\", \"H:\\\\\")\n",
    "    \n",
    "    mel_pred = torch.from_numpy(np.load(pred_mel_path)).float().unsqueeze(0)\n",
    "    mel_target = torch.from_numpy(np.load(pred_mel_path.replace('.mel.npy','.npy'))).float().unsqueeze(0)\n",
    "    mel_pred_dtw = DTW(mel_pred, mel_target, scale_factor = 10, range_= 9)\n",
    "    print(mel_pred.shape)\n",
    "    print(\n",
    "        torch.nn.functional.mse_loss(mel_pred, mel_target),\n",
    "        torch.nn.functional.mse_loss(mel_pred_dtw, mel_target),\n",
    "        sep='\\n')\n",
    "    start_frame = 0\n",
    "    end_frame = 999\n",
    "    plot_data(mel_pred[0][:,start_frame:end_frame].numpy())\n",
    "    plot_data(mel_target[0][:,start_frame:end_frame].numpy())\n",
    "    plot_data(mel_pred_dtw[0][:,start_frame:end_frame].numpy())\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Testing) Timestamps for Model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments = torch.rand(1, 80, 12)\n",
    "sequence = torch.rand(1, 12)\n",
    "dur_frames = torch.histc(torch.argmax(alignments[0], dim=1).float(), min=0, max=sequence.shape[1]-1, bins=sequence.shape[1])# number of frames each letter taken the maximum focus of the model.\n",
    "dur_seconds = dur_frames * (275.625/22050)# convert from frames to seconds\n",
    "end_times = dur_seconds * 0.0# new empty list\n",
    "for i, dur_second in enumerate(dur_seconds): # calculate the end times for each letter.\n",
    "    end_times[i] = end_times[i-1] + dur_second# by adding up the durations of all the letters that go before it\n",
    "start_times = torch.nn.functional.pad(end_times, (1,0))[:-1]# calculate the start times by assuming the next letter starts the moment the last one ends.\n",
    "for i, (dur, start, end) in enumerate(zip(dur_seconds, start_times, end_times)):\n",
    "    print(f\"[Letter {i:02}]\\nDuration:\\t{dur:.3f}\\nStart Time:\\t{start:.3f}\\nEnd Time:\\t{end:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
