{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "D:\\Miniconda\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "from CookieTTS.utils.audio.stft import TacotronSTFT, STFT\n",
    "from CookieTTS.utils.dataset.utils import load_wav_to_torch, DTW\n",
    "\n",
    "import sys\n",
    "sys.path.append('../_4_mtw/waveglow') # add WaveGlow to System path for easier importing\n",
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Initialize WaveGlow and Load Checkpoint/Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shift_spect': 0.0, 'scale_spect': 1.0, 'preceived_vol_scaling': False, 'waveflow': True, 'channel_mixing': 'permute', 'mix_first': False, 'n_flows': 8, 'n_group': 20, 'n_early_every': 16, 'n_early_size': 2, 'memory_efficient': 0.0, 'spect_scaling': False, 'upsample_mode': 'normal', 'WN_config': {'gated_unit': 'GTU', 'n_layers': 8, 'n_channels': 128, 'kernel_size_w': 7, 'kernel_size_h': 7, 'n_layers_dilations_w': None, 'n_layers_dilations_h': 1, 'speaker_embed_dim': 96, 'rezero': False, 'cond_layers': 3, 'cond_activation_func': 'lrelu', 'cond_out_activation_func': True, 'negative_slope': 0.5, 'cond_hidden_channels': 256, 'cond_kernel_size': 1, 'cond_padding_mode': 'zeros', 'seperable_conv': True, 'res_skip': True, 'merge_res_skip': False, 'upsample_mode': 'linear'}, 'n_mel_channels': 160, 'speaker_embed': 96, 'cond_layers': 5, 'cond_activation_func': 'lrelu', 'negative_slope': 0.25, 'cond_hidden_channels': 512, 'cond_output_channels': 256, 'cond_residual': True, 'cond_res_rezero': True, 'cond_padding_mode': 'zeros', 'upsample_first': False, 'cond_kernel_size': 5, 'win_length': 2400, 'hop_length': 600, 'preempthasis': 0.9, 'use_logvar_channels': True, 'load_hidden_from_disk': False, 'iso226_empthasis': False}\n",
      "Config File from 'H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_DTW2\\config.json' successfully loaded.\n",
      "intializing WaveGlow model... Flow 0 using Normal Backprop\n",
      "Flow 1 using Normal Backprop\n",
      "Flow 2 using Normal Backprop\n",
      "Flow 3 using Normal Backprop\n",
      "Flow 4 using Normal Backprop\n",
      "Flow 5 using Normal Backprop\n",
      "Flow 6 using Normal Backprop\n",
      "Flow 7 using Normal Backprop\n",
      "Done!\n",
      "loading WaveGlow checkpoint... Done!\n",
      "initializing Denoiser... Done!\n",
      "WaveGlow trained for 56000 iterations\n"
     ]
    }
   ],
   "source": [
    "# Load WaveGlow\n",
    "def load_waveglow(\n",
    "    waveglow_path = r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_HDN\\best_val_model\",\n",
    "    config_fpath = r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_HDN\\config.json\",):\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    def is_ax(config):\n",
    "        \"\"\"Quickly check if a model uses the Ax WaveGlow core by what's available in the config file.\"\"\"\n",
    "        return True if 'upsample_first' in config.keys() else False\n",
    "    \n",
    "    # Load config file\n",
    "    with open(config_fpath) as f:\n",
    "        data = f.read()\n",
    "    config = json.loads(data)\n",
    "    train_config = config[\"train_config\"]\n",
    "    data_config = config[\"data_config\"]\n",
    "    if 'preempthasis' not in data_config.keys():\n",
    "        data_config['preempthasis'] = 0.0\n",
    "    if 'use_logvar_channels' not in data_config.keys():\n",
    "        data_config['use_logvar_channels'] = False\n",
    "    if 'load_hidden_from_disk' not in data_config.keys():\n",
    "        data_config['load_hidden_from_disk'] = False\n",
    "    if not 'iso226_empthasis' in data_config.keys():\n",
    "        data_config[\"iso226_empthasis\"] = False\n",
    "    dist_config = config[\"dist_config\"]\n",
    "    data_config['n_mel_channels'] = config[\"waveglow_config\"]['n_mel_channels'] if 'n_mel_channels' in config[\"waveglow_config\"].keys() else 160\n",
    "    waveglow_config = {\n",
    "        **config[\"waveglow_config\"],\n",
    "        'win_length': data_config['win_length'],\n",
    "        'hop_length': data_config['hop_length'],\n",
    "        'preempthasis': data_config['preempthasis'],\n",
    "        'n_mel_channels': data_config[\"n_mel_channels\"],\n",
    "        'use_logvar_channels': data_config[\"use_logvar_channels\"],\n",
    "        'load_hidden_from_disk': data_config[\"load_hidden_from_disk\"],\n",
    "        'iso226_empthasis': data_config[\"iso226_empthasis\"]\n",
    "    }\n",
    "    print(waveglow_config)\n",
    "    print(f\"Config File from '{config_fpath}' successfully loaded.\")\n",
    "    \n",
    "    # import the correct model core\n",
    "    if is_ax(waveglow_config):\n",
    "        from CookieTTS._4_mtw.waveglow.efficient_model_ax import WaveGlow\n",
    "    else:\n",
    "        if waveglow_config[\"yoyo\"]:\n",
    "            from CookieTTS._4_mtw.waveglow.efficient_model import WaveGlow\n",
    "        else:\n",
    "            from CookieTTS._4_mtw.waveglow.glow import WaveGlow\n",
    "    from CookieTTS._4_mtw.waveglow.denoiser import Denoiser\n",
    "    \n",
    "    # initialize model\n",
    "    print(f\"intializing WaveGlow model... \", end=\"\")\n",
    "    waveglow = WaveGlow(**waveglow_config).cuda()\n",
    "    print(f\"Done!\")\n",
    "    \n",
    "    # load checkpoint from file\n",
    "    print(f\"loading WaveGlow checkpoint... \", end=\"\")\n",
    "    checkpoint = torch.load(waveglow_path)\n",
    "    waveglow.load_state_dict(checkpoint['model']) # and overwrite initialized weights with checkpointed weights\n",
    "    waveglow.cuda().eval() # move to GPU and convert to half precision\n",
    "    #waveglow.half()\n",
    "    #waveglow.remove_weightnorm()\n",
    "    print(f\"Done!\")\n",
    "    \n",
    "    print(f\"initializing Denoiser... \", end=\"\")\n",
    "    cond_channels = waveglow_config['n_mel_channels']*(waveglow_config['use_logvar_channels']+1)\n",
    "    denoiser = Denoiser(waveglow, n_mel_channels=cond_channels, mu=0.0, var=1.0, stft_device='cpu', speaker_dependant=False)\n",
    "    print(f\"Done!\")\n",
    "    waveglow_iters = checkpoint['iteration']\n",
    "    print(f\"WaveGlow trained for {waveglow_iters} iterations\")\n",
    "    speaker_lookup = checkpoint['speaker_lookup'] # ids lookup\n",
    "    training_sigma = train_config['sigma']\n",
    "    return waveglow, denoiser, speaker_lookup, training_sigma, waveglow_iters, waveglow_config, data_config\n",
    "\n",
    "waveglow, denoiser, speaker_lookup, training_sigma, waveglow_iters, waveglow_config, data_config = load_waveglow(\n",
    "                        waveglow_path = r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_DTW2\\best_val_model\",\n",
    "                        config_fpath = r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_DTW2\\config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Setup STFT to generate wavs from audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing STFT...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Setup for generating Spectrograms from Audio files\n",
    "def load_mel(path):\n",
    "    if path.endswith('.wav') or path.endswith('.flac'):\n",
    "        audio, sampling_rate, max_audio_value = load_wav_to_torch(path)\n",
    "        if sampling_rate != stft.sampling_rate:\n",
    "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "                sampling_rate, stft.sampling_rate))\n",
    "        audio_norm = audio / max_audio_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = stft.mel_spectrogram(audio_norm)\n",
    "    elif path.endswith('.npy'):\n",
    "        melspec = torch.from_numpy(np.load(path)).float()\n",
    "    else:\n",
    "        pass\n",
    "    return melspec\n",
    "\n",
    "print('Initializing STFT...')\n",
    "stft = TacotronSTFT(data_config['filter_length'], data_config['hop_length'], data_config['win_length'],\n",
    "                    data_config['n_mel_channels'], data_config['sampling_rate'], data_config['mel_fmin'],\n",
    "                    data_config['mel_fmax'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Reconstruct Audio from Audio Spectrogram using WaveGlow/Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shift_spect': 0.0, 'scale_spect': 1.0, 'preceived_vol_scaling': False, 'waveflow': True, 'channel_mixing': 'permute', 'mix_first': False, 'n_flows': 8, 'n_group': 20, 'n_early_every': 16, 'n_early_size': 2, 'memory_efficient': 0.0, 'spect_scaling': False, 'upsample_mode': 'normal', 'WN_config': {'gated_unit': 'GTU', 'n_layers': 8, 'n_channels': 256, 'kernel_size_w': 7, 'kernel_size_h': 7, 'n_layers_dilations_w': None, 'n_layers_dilations_h': 1, 'speaker_embed_dim': 0, 'rezero': False, 'transposed_conv_hidden_dim': 256, 'transposed_conv_kernel_size': [2, 3, 5], 'transposed_conv_scales': None, 'cond_layers': 0, 'cond_activation_func': 'lrelu', 'cond_out_activation_func': False, 'negative_slope': 0.5, 'cond_hidden_channels': 256, 'cond_kernel_size': 1, 'cond_padding_mode': 'zeros', 'seperable_conv': True, 'res_skip': True, 'merge_res_skip': False, 'upsample_mode': 'linear'}, 'n_mel_channels': 160, 'speaker_embed': 32, 'cond_layers': 4, 'cond_activation_func': 'lrelu', 'negative_slope': 0.1, 'cond_hidden_channels': 1024, 'cond_output_channels': 1024, 'cond_residual': '1x1conv', 'cond_res_rezero': True, 'cond_kernel_size': 1, 'cond_padding_mode': 'zeros', 'upsample_first': True, 'transposed_conv_hidden_dim': 1024, 'transposed_conv_kernel_size': [4, 9, 5], 'transposed_conv_scales': [2, 3, 5], 'transposed_conv_output_dim': 1024, 'transposed_conv_residual': True, 'transposed_conv_residual_linear': True, 'transposed_conv_res_rezero': True, 'group_conv_output_dim': 4096, 'win_length': 2400, 'hop_length': 600, 'preempthasis': 0.9, 'use_logvar_channels': False, 'load_hidden_from_disk': False, 'iso226_empthasis': False}\n",
      "Config File from 'H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_AEF4.1\\config.json' successfully loaded.\n",
      "intializing WaveGlow model... Flow 0 using Normal Backprop\n",
      "Flow 1 using Normal Backprop\n",
      "Flow 2 using Normal Backprop\n",
      "Flow 3 using Normal Backprop\n",
      "Flow 4 using Normal Backprop\n",
      "Flow 5 using Normal Backprop\n",
      "Flow 6 using Normal Backprop\n",
      "Flow 7 using Normal Backprop\n",
      "Done!\n",
      "loading WaveGlow checkpoint... Done!\n",
      "initializing Denoiser... Done!\n",
      "WaveGlow trained for 296076 iterations\n",
      "Generating Audio from 571 Files...\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_09_35_Celestia_Annoyed__What have you done with the elements of harmony!_.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_34_Cheerilee_Neutral__What do you notice about it__.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_34_Cheerilee_Neutral__What do you notice about it__.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_15_50_Twilight_Anxious Confused__What do you suppose has her so upset_ It's not like her_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_15_50_Twilight_Anxious Confused__What do you suppose has her so upset_ It's not like her_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_07_07_Twilight_Neutral__Don't listen to her, princess.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_14_11_Twilight_Confused__What_.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_57_Scootaloo_Annoyed__And it is too chaos.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_15_58_Twilight_Anxious Whispering__Better pick up the pace Before the stress of this gets the better of all of us.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_13_42_Applejack_Anxious Sad__That just can't be the truth.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_05_28_Twilight_Anxious__Is this about the weather, and the animals' weird behavior_ What's happening out there_ why isn't my magic working_ is there.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_06_40_Celestia_Neutral__You six showed the full potential of the elements, By harnessing the magic of your friendship, to beat a mighty foe_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_06_40_Celestia_Neutral__You six showed the full potential of the elements, By harnessing the magic of your friendship, to beat a mighty foe_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_06_23_Celestia_Neutral__Where the elements are kept inside.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_14_03_Twilight_Neutral__Who were you talking to__.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_14_03_Twilight_Neutral__Who were you talking to__.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_14_17_Twilight_Anxious__Did applejack just.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_10_08_Twilight_Neutral__twists and turns! that's it!_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_10_08_Twilight_Neutral__twists and turns! that's it!_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_06_58_Twilight_Neutral__Princess celestia, you can count on_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_06_58_Twilight_Neutral__Princess celestia, you can count on_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_06_21_Celestia_Neutral__This, is canterlot tower.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_06_46_Celestia_Neutral__Although luna and i once wielded the elements, It is you who now control their power, and it is you who must defeat discord_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_06_46_Celestia_Neutral__Although luna and i once wielded the elements, It is you who now control their power, and it is you who must defeat discord_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_14_08_Applejack_Neutral__Nopony whatsoever.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_47_Cheerilee_Neutral__What do you suppose that represents_.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_36_Apple Bloom_Neutral__It's got an evil claw!_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_36_Apple Bloom_Neutral__It's got an evil claw!_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_08_59_Rarity_Annoyed__I can't believe we're wasting our time Talking to a tacky window_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_08_59_Rarity_Annoyed__I can't believe we're wasting our time Talking to a tacky window_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_05_28_Twilight_Anxious__Is this about the weather, and the animals' weird behavior_ What's happening out there_ why isn't my magic working_ is there_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_05_28_Twilight_Anxious__Is this about the weather, and the animals' weird behavior_ What's happening out there_ why isn't my magic working_ is there_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_57_Scootaloo_Annoyed__And it is too chaos_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_57_Scootaloo_Annoyed__And it is too chaos_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_40_Cheerilee_Neutral__This creature is called a draconequus.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_10_19_Twilight_Neutral__Thanks, princess. we won't let you down.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_55_Scootaloo_Annoyed__Don't call me things i don't know the meaning of_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_00_55_Scootaloo_Annoyed__Don't call me things i don't know the meaning of_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_10_08_Twilight_Neutral__twists and turns! that's it!.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_18_28_Fluttershy_Happy__Not really, In fact, i think i'm awfully lucky To have friends who want me to be the best i can be_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_18_28_Fluttershy_Happy__Not really, In fact, i think i'm awfully lucky To have friends who want me to be the best i can be_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_13_42_Applejack_Anxious Sad__That just can't be the truth_.npy'\n",
      "Saved file has Wrong Shape!\n",
      "Path: 'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_13_42_Applejack_Anxious Sad__That just can't be the truth_.npy'\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_07_09_Twilight_Neutral__We'd be honored to use the elements of harmony again.npy'\n",
      "\n",
      "Audio Path:\n",
      "'H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\\00_18_28_Fluttershy_Happy__Not really, In fact, i think i'm awfully lucky To have friends who want me to be the best i can be.npy'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-adbcba34a943>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# use same Z / random seed during validation so results are more consistent and comparable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m                     \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaveglow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmel_outputs_postnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeaker_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspeaker_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_CPU\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\twibot\\cookieppptts\\CookieTTS\\_4_mtw\\waveglow\\efficient_model_ax.py\u001b[0m in \u001b[0;36minfer\u001b[1;34m(self, spect, speaker_ids, artifact_trimming, sigma, t_scaler, return_CPU)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeaker_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_CPU\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_CPU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0martifact_trimming\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0maudio_trim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0martifact_trimming\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhop_length\u001b[0m \u001b[1;31m# amount of audio to trim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\twibot\\cookieppptts\\CookieTTS\\_4_mtw\\waveglow\\efficient_model_ax.py\u001b[0m in \u001b[0;36minverse\u001b[1;34m(self, z, cond, speaker_ids, return_CPU)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'n_flow_group_conv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mcond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_flow_group_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_flows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# [[B, cond_dim, T//n_group],] * n_flows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mremained_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    206\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    207\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 208\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "waveglow_paths = [\n",
    "#    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_AEF4.2_iso226\\best_model\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_AEF4.1\\best_model\",\n",
    "#    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_AEF\\best_val_model\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_DTW2\\best_val_model\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_6_Flow_512C_ssvae2_2\\best_val_model\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_6_Flow_512C_ssvae2\\best_model\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_DTW\\best_model\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow\\best_val_model_gt3\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_HDN\\best_val_model\",\n",
    "]\n",
    "config_fpaths = [\n",
    "#    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_AEF4.2_iso226\\config.json\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_AEF4.1\\config.json\",\n",
    "#    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_AEF\\config.json\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_DTW2\\config.json\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_6_Flow_512C_ssvae2_2\\config.json\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_6_Flow_512C_ssvae2\\config.json\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_DTW\\config.json\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow\\config_original.json\",\n",
    "    r\"H:\\TTCheckpoints\\waveflow\\4thLargeKernels\\AR_8_Flow_HDN\\config.json\",\n",
    "]\n",
    "output_dirnames = [\n",
    "#    \"AR_8_Flow_AEF4.2_iso226\",\n",
    "    \"AR_8_Flow_AEF4.1_gt\",\n",
    "#    \"AR_8_Flow_AEF_gt\",\n",
    "    \"AR_8_Flow_DTW2\",\n",
    "    \"AR_6_Flow_512C_ssvae2_2\",\n",
    "    \"AR_6_Flow_512C_ssvae2\",\n",
    "    \"AR_8_Flow_DTW\",\n",
    "    \"AR_8_Flow_gt3\",\n",
    "    \"AR_8_Flow_HDN\",\n",
    "]\n",
    "exts = [\n",
    "#    '__*.npy',\n",
    "    '__*.npy',\n",
    "#    '__*.npy',\n",
    "    '__*.mel.npy',\n",
    "    '__*.mel.npy',\n",
    "    '__*.mel.npy',\n",
    "    '__*.mel.npy',\n",
    "    '__*.mel.npy',\n",
    "    '__*.hdn.npy',\n",
    "]\n",
    "\n",
    "folder_paths = [\n",
    "    r\"H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e1\",\n",
    "    r\"H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S2\\s2e2\",\n",
    "    r\"H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S4\\s4e12\",\n",
    "    r\"H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S5\\s5e18\",\n",
    "    r\"H:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S9\\s9e8\",\n",
    "]\n",
    "gt_ext = '.npy'\n",
    "sigmas = [0.0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0,]\n",
    "denoise_strengths = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "speaker_ids = [0,]\n",
    "speaker_ids = [speaker_lookup[x] for x in speaker_ids] # map speaker ids to internel\n",
    "speaker_ids = torch.tensor(speaker_ids).cuda().long()\n",
    "\n",
    "use_DTW = False\n",
    "\n",
    "display_audio = False\n",
    "display_denoised_audio = False\n",
    "\n",
    "save_outputs = True\n",
    "\n",
    "for waveglow_path, config_fpath, output_dirname, ext in list(zip(waveglow_paths, config_fpaths, output_dirnames, exts))[:1]:\n",
    "    waveglow, denoiser, speaker_lookup, training_sigma, waveglow_iters, waveglow_config, data_config = load_waveglow(\n",
    "                                         waveglow_path=waveglow_path, config_fpath=config_fpath)\n",
    "    \n",
    "    output_folder = r\"D:\\Downloads\\infer\\WaveFlow\\\\\" + f\"{output_dirname}\" + f\"_{waveglow_iters}\"\n",
    "    \n",
    "    audio_paths = [glob(os.path.join(folder_path, '**', f'*{ext}'), recursive=True) for folder_path in folder_paths]\n",
    "    audio_paths = [item for sublist in audio_paths for item in sublist]\n",
    "    if ext is '.npy' or '__*.npy':\n",
    "        audio_paths = [x for x in audio_paths if not (x.endswith('.hdn.npy') or x.endswith('.mel.npy') or x.endswith('0.npy') or x.endswith('gdur.npy') or x.endswith('genc_out.npy') or x.endswith('pdur.npy') or x.endswith('penc_out.npy'))]\n",
    "    print(f'Generating Audio from {len(audio_paths)} Files...')\n",
    "    for audio_path in audio_paths:\n",
    "        print(f\"Audio Path:\\n'{audio_path}'\")\n",
    "        mel_outputs_postnet = load_mel(audio_path).cuda()\n",
    "        \n",
    "        if not waveglow_config['use_logvar_channels'] and (mel_outputs_postnet.shape[0] == waveglow.n_mel_channels*2):\n",
    "            mel_outputs_postnet = mel_outputs_postnet.chunk(2, dim=0)[0]\n",
    "            mel_logvars_postnet = None\n",
    "        elif not waveglow_config['use_logvar_channels'] and (mel_outputs_postnet.shape[0] == waveglow.n_mel_channels):\n",
    "            mel_logvars_postnet = None\n",
    "        elif waveglow_config['use_logvar_channels'] and (mel_outputs_postnet.shape[0] == waveglow.n_mel_channels*2):\n",
    "            mel_outputs_postnet, mel_logvars_postnet = mel_outputs_postnet.chunk(2, dim=0)\n",
    "        elif waveglow_config['use_logvar_channels'] and (mel_outputs_postnet.shape[0] == waveglow.n_mel_channels*2):\n",
    "            mel_logvars_postnet = mel_outputs_postnet.new_ones(mel_outputs_postnet.shape) * -4.9\n",
    "        else:\n",
    "            print(f\"Saved file has Wrong Shape!\\nPath: '{audio_path}'\")\n",
    "            continue\n",
    "        if use_DTW and not waveglow_config['load_hidden_from_disk']:\n",
    "            gt_mel_outputs_postnet = load_mel(audio_path.replace(ext, gt_ext)).cuda()\n",
    "            mel_outputs_postnet = DTW(mel_outputs_postnet.unsqueeze(0), gt_mel_outputs_postnet.unsqueeze(0), 8, 7).squeeze(0)\n",
    "        if mel_logvars_postnet is not None:\n",
    "            mel_outputs_postnet = torch.cat((mel_outputs_postnet, mel_logvars_postnet), dim=0)\n",
    "        \n",
    "        output_path = os.path.join(output_folder, os.path.splitext(os.path.split(audio_path)[-1])[0])\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        audios = []\n",
    "        save_path = os.path.join(output_path, 'Ground Truth.wav')\n",
    "        wav_path = audio_path.replace('.hdn.npy','.wav').replace('.mel.npy','.wav').replace('.npy','.wav')\n",
    "        shutil.copy(wav_path, save_path)\n",
    "        with torch.no_grad():\n",
    "            for i, sigma in enumerate(sigmas):\n",
    "                with torch.random.fork_rng(devices=[0,]):\n",
    "                    torch.random.manual_seed(0)# use same Z / random seed during validation so results are more consistent and comparable.\n",
    "                    \n",
    "                    audio = waveglow.infer(mel_outputs_postnet.unsqueeze(0), sigma=sigma, speaker_ids=speaker_ids, return_CPU=True).float().clamp(min=-0.999, max=0.999)\n",
    "                \n",
    "                if (torch.isnan(audio) | torch.isinf(audio)).any():\n",
    "                    print('inf or nan found in audio')\n",
    "                audio[torch.isnan(audio) | torch.isinf(audio)] = 0.0\n",
    "                #audio[:,-1]=1.0\n",
    "                audios.append(audio)\n",
    "                if display_audio:\n",
    "                    ipd.display(ipd.Audio(audio[0].data.cpu().numpy(), rate=data_config['sampling_rate']))\n",
    "                if save_outputs:\n",
    "                    save_path = os.path.join(output_path, f'denoise_{0.00:0.2f}_sigma_{sigma:0.2f}.wav')\n",
    "                    write(save_path, data_config['sampling_rate'], (audio[0]* 2**15).data.cpu().numpy().astype('int16'))\n",
    "            \n",
    "            for i, (audio, sigma) in enumerate(zip(audios, sigmas)):\n",
    "                for denoise_strength in denoise_strengths:\n",
    "                    audio_denoised = denoiser(audio, speaker_ids=speaker_ids, strength=denoise_strength)[:, 0]\n",
    "                    if (torch.isnan(audio) | torch.isinf(audio)).any():\n",
    "                        print('inf or nan found in audio')\n",
    "                    assert (not torch.isinf(audio_denoised).any()) or (not torch.isnan(audio_denoised).any())\n",
    "                    #print(f\"[Denoised Strength {denoise_strength}] [sigma {sigma}]\")\n",
    "                    if display_denoised_audio:\n",
    "                        ipd.display(ipd.Audio(audio_denoised.cpu().numpy(), rate=data_config['sampling_rate']))\n",
    "                    if save_outputs:\n",
    "                        save_path = os.path.join(output_path, f'denoise_{denoise_strength:0.2f}_sigma_{sigma:0.2f}.wav')\n",
    "                        write(save_path, data_config['sampling_rate'], (audio_denoised[0]* 2**15).data.cpu().numpy().astype('int16'))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Testing) Blending GT and Pred Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "min_ = 110\n",
    "max_ = 120\n",
    "n_mel_channels = 160\n",
    "gt_perc = ((torch.arange(1, n_mel_channels+1).float()-min_).clamp(0)/(max_-min_)).clamp(max=1.0)\n",
    "print(gt_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Testing) Dynamic Time Warping for GTA Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "target = torch.rand(1, 2, 700)\n",
    "pred = torch.rand(1, 2, 700)\n",
    "\n",
    "@torch.jit.script\n",
    "def DTW(batch_pred, batch_target, scale_factor: int, range_: int):\n",
    "    \"\"\"\n",
    "    Calcuates ideal time-warp for each frame to minimize L1 Error from target.\n",
    "    Params:\n",
    "        scale_factor: Scale factor for linear interpolation.\n",
    "                      Values greater than 1 allows blends neighbouring frames to be used.\n",
    "        range_: Range around the target frame that predicted frames should be tested as possible candidates to output.\n",
    "                If range is set to 1, then predicted frames with more than 0.5 distance cannot be used. (where 0.5 distance means blending the 2 frames together).\n",
    "    \"\"\"\n",
    "    assert range_ % 2 == 1, 'range_ must be an odd integer.'\n",
    "    assert batch_pred.shape == batch_target.shape, 'pred and target shapes do not match.'\n",
    "    \n",
    "    batch_pred_dtw = batch_pred * 0.\n",
    "    for i, (pred, target) in enumerate(zip(batch_pred, batch_target)):\n",
    "        pred = pred.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "        \n",
    "        # shift pred into all aligned forms that might produce improved L1\n",
    "        pred_pad = torch.nn.functional.pad(pred, (range_//2, range_//2))\n",
    "        pred_expanded = torch.nn.functional.interpolate(pred_pad, scale_factor=float(scale_factor), mode='linear', align_corners=False)# [B, C, T] -> [B, C, T*s]\n",
    "        \n",
    "        p_shape = pred.shape\n",
    "        pred_list = []\n",
    "        for j in range(scale_factor*range_):\n",
    "            pred_list.append(pred_expanded[:,:,j::scale_factor][:,:,:p_shape[2]])\n",
    "        \n",
    "        pred_dtw = pred.clone()\n",
    "        for pred_interpolated in pred_list:\n",
    "            new_l1 = torch.nn.functional.l1_loss(pred_interpolated, target, reduction='none').sum(dim=1, keepdim=True)\n",
    "            old_l1 = torch.nn.functional.l1_loss(pred_dtw, target, reduction='none').sum(dim=1, keepdim=True)\n",
    "            pred_dtw = torch.where(new_l1 < old_l1, pred_interpolated, pred_dtw)\n",
    "        batch_pred_dtw[i:i+1] = pred_dtw\n",
    "    return batch_pred_dtw\n",
    "\n",
    "pred_dtw = DTW(pred, target, 4, 3)\n",
    "print(torch.nn.functional.l1_loss(pred, target))\n",
    "print(torch.nn.functional.l1_loss(pred_dtw, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "def plot_data(data, title=None, figsize=(20, 5)):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.imshow(data, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6703)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(-0.4).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "filetext = open(r\"G:\\TwiBot\\CookiePPPTTS\\CookieTTS\\_2_ttm\\tacotron2\\GTA_flist2\\map_val.txt\", \"r\").read().split(\"\\n\")\n",
    "filter_str = [\".mel100\",\".mel200\",\".mel300\",\".mel400\",\".mel500\"]\n",
    "filetext = [x for x in filetext if not any(str_ in x for str_ in filter_str)]\n",
    "\n",
    "rand_start = int(random.random()*len(filetext))-file_count\n",
    "rand_start = 10\n",
    "file_count = 20\n",
    "for line in filetext[rand_start:rand_start+file_count]:\n",
    "    pred_mel_path = line.split(\"|\")[1].replace(\"\\n\",\"\").replace(\"/media/cookie/Samsung 860 QVO/\", \"H:\\\\\")\n",
    "    \n",
    "    mel_pred = torch.from_numpy(np.load(pred_mel_path)).float().unsqueeze(0)\n",
    "    mel_pred[:, 120:, :] = 0.0\n",
    "    mel_target = torch.from_numpy(np.load(pred_mel_path.replace('.mel.npy','.npy'))).float().unsqueeze(0)\n",
    "    mel_target[:, 120:, :] = 0.0\n",
    "    mel_pred_dtw = DTW(mel_pred, mel_target, scale_factor = 8, range_= 7)\n",
    "    print(mel_pred.shape)\n",
    "    print(\n",
    "        torch.nn.functional.mse_loss(mel_pred, mel_target),\n",
    "        torch.nn.functional.mse_loss(mel_pred_dtw, mel_target),\n",
    "        sep='\\n')\n",
    "    start_frame = 0\n",
    "    end_frame = 999\n",
    "    plot_data(mel_pred[0][:,start_frame:end_frame].numpy())\n",
    "    plot_data(mel_target[0][:,start_frame:end_frame].numpy())\n",
    "    plot_data(mel_pred_dtw[0][:,start_frame:end_frame].numpy())\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Testing) Timestamps for Model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments = torch.rand(1, 80, 12)\n",
    "sequence = torch.rand(1, 12)\n",
    "dur_frames = torch.histc(torch.argmax(alignments[0], dim=1).float(), min=0, max=sequence.shape[1]-1, bins=sequence.shape[1])# number of frames each letter taken the maximum focus of the model.\n",
    "dur_seconds = dur_frames * (275.625/22050)# convert from frames to seconds\n",
    "end_times = dur_seconds * 0.0# new empty list\n",
    "for i, dur_second in enumerate(dur_seconds): # calculate the end times for each letter.\n",
    "    end_times[i] = end_times[i-1] + dur_second# by adding up the durations of all the letters that go before it\n",
    "start_times = torch.nn.functional.pad(end_times, (1,0))[:-1]# calculate the start times by assuming the next letter starts the moment the last one ends.\n",
    "for i, (dur, start, end) in enumerate(zip(dur_seconds, start_times, end_times)):\n",
    "    print(f\"[Letter {i:02}]\\nDuration:\\t{dur:.3f}\\nStart Time:\\t{start:.3f}\\nEnd Time:\\t{end:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
